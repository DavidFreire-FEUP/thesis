\chapter{Conclusion and Future Work} 
\label{chap:conclusions}

\section{Summary and Discussion of Thesis Results}

The work developed in this thesis, aims to achieve a deterministic approach as
for the number of consumers required working in parallel, so as to guarantee
that the rate of production into a topic is not higher than the rate of
consumption, while minimizing the operational cost.

The goal was to deterministically solve the autoscaling problem related to a
group of consumers, which we model as a Bin Packing Problem where the items'
sizes and bin assignments can change over time. In light of these variations,
items' assignments can be rebalanced (bin assignment can change), and therefore
we propose the Rscore to account for the rebalance cost, Section
\ref{sub:rscore}.

Given the fact that, to the best of our knowledge, it is the first time the BPP
is applied in these conditions, there was lack of heuristic algorithms that
solve the Bin Packing assignment while considering the Rscore. As such, in
Section \ref{subsub:modified_any_fit} we propose four new heuristic algorithms
based on the Rscore, three of which are proven to be competitive solutions in
Section \ref{subsub:test_results}, when solving the multi-objective optimization
problem.

Furthermore, in Chapter \ref{chap:consumer_group_autoscaler}, a system was
developed to automatically scale a group of consumers and manage the partitions'
assignments based on the aformentioned theoretical approach. This system was
integrated in the company's infrastructure, and as expected is capable of
providing a solution that scales a group of consumers based on the system's
current load.

\section{Future Work}

Given the multiple parts developed within this system, additional improvements
could be achieved. Most notably, due to the close relationship between the system
and the Kubernetes cluster, the autoscaler could be wrapped as a Kubernetes
Operator \cite{KuberenetesOperator} as is common with systems of this kind
(\cite{Kubegres, PulumiOperator, KEDA}).

\subsection{Monitor}

To the best of our knowledge, for lack of a better metric being provided by the
Kafka cluster, the value that was used to track the write speed of each
partition was the number of bytes in each partition. For this reason, a process
(Section \ref{component:Monitor}) was used to evaluate the speed of each partition by
historically storing the amount of bytes in a partition at a certain timestamp.

A clear disadvantage of using this component is the fact that if
\lstinline{retention.ms != -1}, a record is deleted from its partition after
\lstinline{retention.ms} which leads to a reduction in the amount of bytes in
the partition, consequently having the monitor process erroneously evaluating a
negative partition write speed. To circumvent this behaviour, either one of the
following metrics provided by Kafka would suffice: The current write speed
(bytes/s) per partition; A historic amount of bytes that have been written to a
partition. Since the monitor process presents the write speed as the average
data rate over the last 30 seconds, using a different strategy to evaluate the
speed could lead to more robust measures and improved convergence times.

\subsection{Consumer}

If the network is experiencing increased latency, the current implementation of
the autoscaler does not account for changes in the size of the consumer
capacity, therefore the controller assumes a maximum capacity of a consumer
which is not accurate within this scenario. Metrics related to a consumer's
performance provided by Kafka JMX metrics could be leveraged to obtain a more
accurate dynamic representation of the consumer's current capacity. This could
also lead to a variable bin size BPP.

\subsection{Controller}

Throughout this thesis, the controller process was presented with strict
execution rules and actions to manage the Kafka consumer group. Evolving this
component into a more abstract concept, could not only maintain its current
functionalities, but it could also be used to more accurately solve load
balancing within a consumer group without necessarily modeling the problem as a
BPP. 

In fact, the heuristic algorithms that make use of the worst fit rule when
assigning partitions to consumers, are at the same time applying a load balancer
rule between the available consumers.

\section{Final Remarks}

In spite of the fact that, as a whole, the thesis presents quite a specific use
case, individually several of Kafka's functionalities are challenged and tested
alternatives are also provided which can be used as a foundation so as to
improve the way in which a consumer's load is modeled, and the manner in which
the load (partitions) is distributed between the elements of a consumer group.
